#!/usr/bin/env python3
"""
TACOæ•°æ®é›†æ‹†åˆ†è„šæœ¬ - ä½¿ç”¨VERLå®˜æ–¹æ–¹å¼
åŸºäºHugging Face datasetsçš„åŸç”ŸsplitåŠŸèƒ½æ‹†åˆ†æ•°æ®é›†ï¼Œä¸VERLå®˜æ–¹ç¤ºä¾‹ä¿æŒä¸€è‡´
"""

import argparse
import os
from pathlib import Path
from datasets import load_dataset
import json

def split_taco_dataset(input_file: str, output_dir: str = "./", train_ratio: float = 0.8):
    """
    ä½¿ç”¨VERLå®˜æ–¹æ–¹å¼æ‹†åˆ†TACOæ•°æ®é›†
    
    Args:
        input_file: è¾“å…¥çš„Parquetæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹ (0.0 - 1.0)
    """
    input_path = Path(input_file)
    output_path = Path(output_dir)
    
    # ç¡®ä¿è¾“å…¥æ–‡ä»¶å­˜åœ¨
    if not input_path.exists():
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“– è¯»å–æ•°æ®é›†: {input_path}")
    
    # ä½¿ç”¨datasetsåŠ è½½Parquetæ–‡ä»¶
    try:
        dataset = load_dataset('parquet', data_files=str(input_path), split='train')
        total_size = len(dataset)
        print(f"âœ… æˆåŠŸè¯»å– {total_size} æ¡æ•°æ®")
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # è®¡ç®—æ‹†åˆ†æ¯”ä¾‹
    train_percentage = int(train_ratio * 100)
    test_percentage = 100 - train_percentage
    train_size = int(total_size * train_ratio)
    test_size = total_size - train_size
    
    print(f"ğŸ“Š æ•°æ®é›†æ‹†åˆ†ä¿¡æ¯:")
    print(f"   æ€»æ•°æ®é‡: {total_size}")
    print(f"   è®­ç»ƒé›†: {train_size} ({train_ratio:.1%})")
    print(f"   æµ‹è¯•é›†: {test_size} ({1-train_ratio:.1%})")
    
    # ä½¿ç”¨VERLå®˜æ–¹æ–¹å¼è¿›è¡Œæ‹†åˆ†
    print(f"ğŸ”€ å¼€å§‹æ‹†åˆ†æ•°æ®é›†...")
    
    try:
        # ä½¿ç”¨datasetsçš„splitåŠŸèƒ½ï¼Œç±»ä¼¼å®˜æ–¹ç¤ºä¾‹
        train_dataset = load_dataset('parquet', data_files=str(input_path), split=f'train[:{train_percentage}%]')
        test_dataset = load_dataset('parquet', data_files=str(input_path), split=f'train[-{test_percentage}%:]')
        
        print(f"âœ… æ‹†åˆ†å®Œæˆ")
        print(f"   è®­ç»ƒé›†å®é™…å¤§å°: {len(train_dataset)}")
        print(f"   æµ‹è¯•é›†å®é™…å¤§å°: {len(test_dataset)}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ‹†åˆ†å¤±è´¥: {e}")
        return
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    base_name = input_path.stem  # ä¸åŒ…å«æ‰©å±•åçš„æ–‡ä»¶å
    train_file = output_path / f"{base_name}_train.parquet"
    test_file = output_path / f"{base_name}_test.parquet"
    
    # ä¿å­˜è®­ç»ƒé›†
    print(f"ğŸ’¾ ä¿å­˜è®­ç»ƒé›†: {train_file}")
    try:
        train_dataset.to_parquet(str(train_file))
        print(f"âœ… è®­ç»ƒé›†ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ è®­ç»ƒé›†ä¿å­˜å¤±è´¥: {e}")
        return
    
    # ä¿å­˜æµ‹è¯•é›†
    print(f"ğŸ’¾ ä¿å­˜æµ‹è¯•é›†: {test_file}")
    try:
        test_dataset.to_parquet(str(test_file))
        print(f"âœ… æµ‹è¯•é›†ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æµ‹è¯•é›†ä¿å­˜å¤±è´¥: {e}")
        return
    
    # ä¿å­˜æ‹†åˆ†ä¿¡æ¯
    split_info = {
        "input_file": str(input_path),
        "total_samples": total_size,
        "train_samples": len(train_dataset),
        "test_samples": len(test_dataset),
        "train_ratio": train_ratio,
        "test_ratio": 1 - train_ratio,
        "train_file": str(train_file),
        "test_file": str(test_file),
        "split_method": "huggingface_datasets_percentage",
        "note": "ä½¿ç”¨VERLå®˜æ–¹æ–¹å¼è¿›è¡Œæ•°æ®æ‹†åˆ†"
    }
    
    info_file = output_path / f"{base_name}_split_info.json"
    print(f"ğŸ“„ ä¿å­˜æ‹†åˆ†ä¿¡æ¯: {info_file}")
    try:
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(split_info, f, indent=2, ensure_ascii=False)
        print(f"âœ… æ‹†åˆ†ä¿¡æ¯ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ æ‹†åˆ†ä¿¡æ¯ä¿å­˜å¤±è´¥: {e}")
    
    print(f"\nğŸ‰ æ•°æ®é›†æ‹†åˆ†å®Œæˆ!")
    print(f"   è®­ç»ƒé›†: {train_file}")
    print(f"   æµ‹è¯•é›†: {test_file}")
    print(f"   ä¿¡æ¯æ–‡ä»¶: {info_file}")

def validate_split(train_file: str, test_file: str):
    """
    éªŒè¯æ‹†åˆ†ç»“æœçš„æ­£ç¡®æ€§
    
    Args:
        train_file: è®­ç»ƒé›†æ–‡ä»¶è·¯å¾„
        test_file: æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„
    """
    print(f"\nğŸ” éªŒè¯æ‹†åˆ†ç»“æœ...")
    
    try:
        # ä½¿ç”¨datasetsè¯»å–æ‹†åˆ†åçš„æ–‡ä»¶
        train_dataset = load_dataset('parquet', data_files=train_file, split='train')
        test_dataset = load_dataset('parquet', data_files=test_file, split='train')
        
        print(f"âœ… éªŒè¯é€šè¿‡:")
        print(f"   è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        print(f"   æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
        print(f"   æ€»å¤§å°: {len(train_dataset) + len(test_dataset)}")
        
        # æ£€æŸ¥åˆ—æ˜¯å¦ä¸€è‡´
        train_features = list(train_dataset.features.keys())
        test_features = list(test_dataset.features.keys())
        
        if train_features == test_features:
            print(f"âœ… åˆ—ç»“æ„ä¸€è‡´: {len(train_features)} åˆ—")
            print(f"   åˆ—å: {train_features}")
        else:
            print(f"âš ï¸ åˆ—ç»“æ„ä¸ä¸€è‡´")
            print(f"   è®­ç»ƒé›†åˆ—: {train_features}")
            print(f"   æµ‹è¯•é›†åˆ—: {test_features}")
        
        # æ˜¾ç¤ºéƒ¨åˆ†ç¤ºä¾‹æ•°æ®
        print(f"\nğŸ“‹ è®­ç»ƒé›†å‰3è¡Œç¤ºä¾‹:")
        for i in range(min(3, len(train_dataset))):
            example = train_dataset[i]
            print(f"   æ ·æœ¬ {i+1}:")
            print(f"     data_source: {example.get('data_source', 'N/A')}")
            if 'extra_info' in example and example['extra_info']:
                extra_info = example['extra_info']
                print(f"     id: {extra_info.get('id', 'N/A')}")
                print(f"     difficulty: {extra_info.get('difficulty', 'N/A')}")
            print()
        
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°ï¼Œå¤„ç†å‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨VERLå®˜æ–¹æ–¹å¼å°†TACOæ•°æ®é›†æŒ‰æ¯”ä¾‹æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        "--input-file", 
        type=str, 
        default="/mnt/chensiheng/shuotang/git_repo_h200/AI-Researcher/verl/rl_data_process/taco_dataset/result/taco_rl.parquet",
        help="è¾“å…¥çš„TACO Parquetæ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--output-dir", 
        type=str, 
        default="/mnt/chensiheng/shuotang/git_repo_h200/AI-Researcher/verl/rl_data_process/taco_dataset/result/",
        help="è¾“å‡ºç›®å½•è·¯å¾„"
    )
    
    parser.add_argument(
        "--train-ratio", 
        type=float, 
        default=0.8,
        help="è®­ç»ƒé›†æ¯”ä¾‹ (0.0-1.0)"
    )
    
    parser.add_argument(
        "--validate", 
        action="store_true",
        help="æ‹†åˆ†åéªŒè¯ç»“æœ"
    )
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if not (0.0 < args.train_ratio < 1.0):
        print(f"âŒ é”™è¯¯: è®­ç»ƒé›†æ¯”ä¾‹å¿…é¡»åœ¨ 0.0 å’Œ 1.0 ä¹‹é—´ï¼Œå½“å‰å€¼: {args.train_ratio}")
        return
    
    # æ‰§è¡Œæ‹†åˆ†
    try:
        split_taco_dataset(
            input_file=args.input_file,
            output_dir=args.output_dir,
            train_ratio=args.train_ratio
        )
        
        # å¯é€‰éªŒè¯
        if args.validate:
            base_name = Path(args.input_file).stem
            output_path = Path(args.output_dir)
            train_file = output_path / f"{base_name}_train.parquet"
            test_file = output_path / f"{base_name}_test.parquet"
            validate_split(str(train_file), str(test_file))
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()