#!/usr/bin/env python3
"""
å°†TACOæ•°æ®é›†ï¼ˆJSONæ ¼å¼ï¼‰è½¬æ¢ä¸ºVERLå¼ºåŒ–å­¦ä¹ æ¡†æ¶æ‰€éœ€çš„Parquetæ ¼å¼ã€‚

æ­¤è„šæœ¬æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
1.  ä»¥æµå¼æ–¹å¼è¯»å–æºJSONæ–‡ä»¶ï¼Œä»¥é«˜æ•ˆå¤„ç†å¤§æ–‡ä»¶ã€‚
2.  ä¸ºæ¯ä¸ªé—®é¢˜æ·»åŠ ä¸€ä¸ªæ ‡å‡†çš„æŒ‡ä»¤æ€§å‰ç¼€ï¼ŒæŒ‡å¯¼æ¨¡å‹ç”Ÿæˆæ ¼å¼æ­£ç¡®çš„ä»£ç ã€‚
3.  å°†æ¯ä¸ªJSONå¯¹è±¡é‡æ„ä¸ºRLHFDatasetå…¼å®¹çš„æ ¼å¼ï¼ŒåŒ…æ‹¬ï¼š
    - `prompt`: åŒ…å«è§’è‰²å’Œå†…å®¹çš„èŠå¤©æ ¼å¼åˆ—è¡¨ã€‚
    - `data_source`: ç¡¬ç¼–ç ä¸º "taco"ã€‚
    - `extra_info`: åŒ…å«ç”¨äºå·¥å…·æ‰§è¡Œçš„ `tools_kwargs` (å†…å« `input_outputs`)
      ä»¥åŠå…¶ä»–å…ƒæ•°æ®ï¼ˆå¦‚id, difficultyï¼‰ã€‚
4.  å°†è½¬æ¢åçš„æ•°æ®ä¿å­˜ä¸ºParquetæ–‡ä»¶ã€‚
"""

import argparse
import json
from pathlib import Path
from typing import List, Dict, Any

import datasets
import ijson
from tqdm import tqdm

# æŒ‡å¯¼æ¨¡å‹ä»£ç ç”Ÿæˆæ ¼å¼çš„æŒ‡ä»¤å‰ç¼€
PROMPT_PREFIX = """You are an expert programmer. Your task is to solve the given algorithmic problem.

Follow this structured approach:

1. **Think through the problem**: Wrap your reasoning in <think></think> tags.

2. **Develop your solution iteratively**:
   - Use <code></code> tags to provide a complete Python function that you want to test and get feedback on.
   - After each code submission, you will receive execution results in <execution_results></execution_results> tags.
   - You can submit multiple <code></code> blocks to iteratively improve your solution based on the test feedback.

3. **Provide your final solution**: When you are confident in your solution, put it in <answer></answer> tags to indicate completion.

**Important formatting rules**:
- All tags must appear in pairs: <think></think>, <code></code>, <answer></answer>
- Every function you provide (whether in <code> or <answer>) must be complete and follow these rules:
  * It must be a single, complete Python function definition
  * It must accept a single string argument containing the test case input
  * It must return a string result which is the solution for the test case  
  * Do not include any code outside the function definition (no test cases, prints, etc.)

Here is the problem description:
---
"""

def transform_item(item: Dict[str, Any]) -> Dict[str, Any]:
    """
    å°†å•ä¸ªTACO JSONå¯¹è±¡è½¬æ¢ä¸ºVERL RLæ•°æ®é›†æ ¼å¼çš„å­—å…¸ã€‚
    """
    question = item.get("question")
    input_output = item.get("input_output")

    if not question or not input_output:
        return None

    # æ„å»ºå®Œæ•´çš„prompt
    full_prompt_content = f"{PROMPT_PREFIX}\n{question}"

    # æ„å»ºRLæ•°æ®é›†è¡Œ
    new_row = {
        "prompt": [{"role": "user", "content": full_prompt_content}],
        "data_source": "taco",
        "reward_model": {
            "style": "rule",  # TACO uses rule-based scoring
            "ground_truth": input_output  # The input/output test cases for evaluation
        },
        "extra_info": {
            "id": item.get("id"),
            "difficulty": item.get("difficulty"),
            "source": item.get("source"),
            "tools_kwargs": {
                # ToolClientå°†ä½¿ç”¨æ­¤å­—æ®µæ¥è·å–æµ‹è¯•ç”¨ä¾‹
                "input_outputs": input_output
            }
        }
    }
    return new_row

def convert_taco_to_rl_format(source_path: Path, output_path: Path, batch_size: int = 1000):
    """
    è¯»å–taco_verified.jsonï¼Œå°†å…¶è½¬æ¢ä¸ºtaco_rl.parquetã€‚
    ä½¿ç”¨åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜æº¢å‡ºã€‚
    """
    if not source_path.exists():
        print(f"âŒ Error: Source file not found at {source_path}")
        return

    print(f"ğŸš€ Starting conversion from '{source_path.name}' to '{output_path.name}'...")
    print(f"ğŸ“¦ Batch size: {batch_size} items per batch")

    processed_count = 0
    skipped_count = 0
    batch_count = 0
    current_batch = []

    # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜å‚¨æ‰¹æ¬¡æ–‡ä»¶
    temp_dir = output_path.parent / "temp_batches"
    temp_dir.mkdir(exist_ok=True)
    
    batch_files = []

    try:
        with source_path.open('rb') as f:
            print("â³ Parsing JSON file and processing in batches...")
            # å‡è®¾JSONçš„é¡¶å±‚æ˜¯ä¸€ä¸ªå¯¹è±¡æ•°ç»„
            parser = ijson.items(f, 'item')
            
            for item in tqdm(parser, desc="Processing TACO items"):
                transformed = transform_item(item)
                if transformed:
                    current_batch.append(transformed)
                    processed_count += 1
                else:
                    skipped_count += 1
                
                # å½“è¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶ï¼Œä¿å­˜å½“å‰æ‰¹æ¬¡
                if len(current_batch) >= batch_size:
                    batch_file = temp_dir / f"batch_{batch_count:04d}.parquet"
                    batch_files.append(batch_file)
                    
                    print(f"ğŸ’¾ Saving batch {batch_count + 1} ({len(current_batch)} items) to {batch_file.name}")
                    hf_batch = datasets.Dataset.from_list(current_batch)
                    hf_batch.to_parquet(batch_file)
                    
                    # æ¸…ç©ºå½“å‰æ‰¹æ¬¡é‡Šæ”¾å†…å­˜
                    current_batch = []
                    batch_count += 1
                    
                    # å¼ºåˆ¶åƒåœ¾å›æ”¶
                    import gc
                    gc.collect()
            
            # å¤„ç†æœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡
            if current_batch:
                batch_file = temp_dir / f"batch_{batch_count:04d}.parquet"
                batch_files.append(batch_file)
                
                print(f"ğŸ’¾ Saving final batch {batch_count + 1} ({len(current_batch)} items) to {batch_file.name}")
                hf_batch = datasets.Dataset.from_list(current_batch)
                hf_batch.to_parquet(batch_file)
                batch_count += 1
                
    except ijson.JSONError as e:
        print(f"âŒ A fatal error occurred during JSON parsing: {e}")
        print("   The file might be corrupted or not a valid JSON array.")
        return
    except Exception as e:
        print(f"âŒ An unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()
        return

    if not batch_files:
        print("ğŸ¤· No data was processed. The output file will not be created.")
        return

    print(f"\nâœ… Successfully processed {processed_count} items in {batch_count} batches.")
    if skipped_count > 0:
        print(f"âš ï¸ Skipped {skipped_count} items due to missing 'question' or 'input_output' fields.")

    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶
    print(f"ğŸ”„ Merging {len(batch_files)} batch files into final dataset...")
    try:
        # é€ä¸ªè¯»å–æ‰¹æ¬¡æ–‡ä»¶å¹¶åˆå¹¶
        all_datasets = []
        for i, batch_file in enumerate(batch_files):
            print(f"ğŸ“– Loading batch {i + 1}/{len(batch_files)}: {batch_file.name}")
            batch_dataset = datasets.load_dataset('parquet', data_files=str(batch_file), split='train')
            all_datasets.append(batch_dataset)
        
        print("ğŸ”— Concatenating all batches...")
        final_dataset = datasets.concatenate_datasets(all_datasets)
        
        print(f"ğŸ’¾ Saving final dataset to: {output_path}")
        final_dataset.to_parquet(output_path)
        
        print("ğŸ§¹ Cleaning up temporary files...")
        for batch_file in batch_files:
            batch_file.unlink()
        temp_dir.rmdir()
        
        print("\nğŸ‰ Conversion complete!")
        print(f"Output file: {output_path}")
        print(f"Final dataset size: {len(final_dataset)} items")
        
    except Exception as e:
        print(f"âŒ Failed to merge batches or save final file: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ä¸»å‡½æ•°ï¼Œå¤„ç†å‘½ä»¤è¡Œå‚æ•°å¹¶å¯åŠ¨è½¬æ¢ã€‚"""
    parser = argparse.ArgumentParser(
        description="Convert TACO JSON dataset to Parquet format for VERL RL training.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        "--source-file",
        type=str,
        default="/mnt/chensiheng/ai_researcher_data/TACO-verified/taco_verified.json",
        help="Path to the source TACO JSON file."
    )
    parser.add_argument(
        "--output-file",
        type=str,
        default="/mnt/chensiheng/shuotang/git_repo_h200/AI-Researcher/verl/rl_data_process/taco_dataset/result/taco_rl.parquet",
        help="Path to save the output Parquet file."
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="Number of items to process per batch (reduce if running out of memory)."
    )
    args = parser.parse_args()

    source_path = Path(args.source_file)
    output_path = Path(args.output_file)

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    convert_taco_to_rl_format(source_path, output_path, args.batch_size)

if __name__ == "__main__":
    main() 